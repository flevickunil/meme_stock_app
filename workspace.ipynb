{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Selenium, generally slower and more resource-intensive compared to other scraping methods\n",
    "* Scrapy is less intensive than selenium, will use\n",
    "* cannot directly simulate clicking through a website using only Requests and BeautifulSoup \n",
    "* Requests is an HTTP library used for making HTTP requests to web servers and fetching HTML content\n",
    "* BeautifulSoup is an HTML parsing library used for extracting data from HTML documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "* Scrapy usually configured via internal scrapy project.\n",
    "* As my web scraper is part of a larger project, need to customise scrapy to run in a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import yfinance as yf\n",
    "from plotly import graph_objs as go\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxx = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "modelxx.save(\"word2vec.model\")\n",
    "modelxx.train([[\"hello\", \"world\"]], total_examples=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id='HM0EZT9M6wIgyBKGqwdeHA', client_secret='jEbsMOFcOIBnibpKoFKoLYcJtmYzxA', user_agent='mac.os:Adv_prog_project:v.1 (by /u/Adv_Prog_proj_user)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "For a submission this would be useful:\n",
    "* author\n",
    "* number of comments\n",
    "* upvote_ratio\n",
    "* comments\n",
    "* date of submission .created_utc\n",
    "* maybe\n",
    "  * awards? theres a lot and it is downlaoded like a dictionary\n",
    "\n",
    "From comments:\n",
    "* author .author\n",
    "* comment score (upvote) .score\n",
    "* comment text .body\n",
    "* date of comment .created_utc \n",
    "\n",
    "Notes:\n",
    "1. comment.body only sees comments on submission, not subcomments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download all submissions/posts\n",
    "\n",
    "n.b Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Parse daily discussion posts to access the comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "author .author\n",
    "* comment score (upvote) .score\n",
    "* comment text .body\n",
    "* date of comment .created_utc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blacklist = ['LLC', 'SEC', 'UK', 'USA', 'US', 'CEO', 'UCLA', 'US', 'OTC', 'I', 'TO', 'AI', 'WSB', 'A', 'GPU', 'TLDR', 'GOING', 'UP', 'ARE', 'SO', 'THE', 'MOON', 'US', 'OP', 'I', 'PDF']\n",
    "\n",
    "timezone = 'Europe/Zurich'\n",
    "\n",
    "def get_api_limit(timezone: str):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        identify how many API calls left in the limit imposed by reddit (600 per 10 minutes) \n",
    "\n",
    "    Arguments:\n",
    "        timezone: timezone as a string e.g. ('Europe/Zurich')\n",
    "\n",
    "    Output:\n",
    "        a print statement informing when the reset time was for API calls and how many remaining requests there are\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #connect to reddit authority limits\n",
    "        limit = reddit.auth.limits\n",
    "        remaining_requests = limit['remaining']\n",
    "        #get datetime of when reset will occur & translate into local time\n",
    "        reset = datetime.utcfromtimestamp(limit['reset_timestamp'])\n",
    "        reset_time = pytz.utc.localize(reset).astimezone(pytz.timezone(timezone))\n",
    "        print(f'Reset time was {reset_time}, you have {remaining_requests} requests remaining')\n",
    "        \n",
    "    except pytz.UnknownTimeZoneError:\n",
    "        print(f'Unknown timezone: {timezone}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on functions:\n",
    "1. get_tickers: preprocess function, then get tickers\n",
    "   1. have option make lowercase = True / False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_reddit import RedditSubmissions, RedditAPIHelper\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the .env file\n",
    "load_dotenv('credentials.env')\n",
    "\n",
    "# Get the credentials from the environment variables\n",
    "user_agent = os.getenv('USER_AGENT')\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit API instance\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "# Create an instance of RedditSubmissions with the Reddit API instance\n",
    "reddit_submissions = RedditSubmissions(reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = reddit_submissions.get_submissions('wallstreetbets',sort = 'top', limit=30, time_filter='year')\n",
    "submission_data = reddit_submissions.extract_submission_data(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise the words from each post in the dataframe\n",
    "modelutils = ModelUtils()\n",
    "submission_data['tickers'] = submission_data['all_text'].apply(lambda x: modelutils.preprocess_text(x))\n",
    "\n",
    "# is the input to the model a list of strings?\n",
    "#check if submission_data['ticker_freq'] is a list of string, check if inputs to other models are also strings\n",
    "#type(submission_data['tickers'][0])\n",
    "\n",
    "#load company data\n",
    "company_data = modelutils.get_company_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup fuzz model dictionaries\n",
    "fuzz_model = FuzzModel()\n",
    "title_to_ticker, ticker_to_title, combined_dict = fuzz_model.create_fuzz_dicts(company_data)\n",
    "values_to_key, values = fuzz_model.fuzz_preprocess(combined_dict)\n",
    "\n",
    "#run the extraction model on the tokenised words\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: run_fuzz_model(tokens, values_to_key, values, 95))\n",
    "\n",
    "#Get top x tickers from each submission\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: TickerFrequencyProcessor.top_tickers(tokens,5))\n",
    "\n",
    "# create ticker df for streamlit graph\n",
    "ticker_dataframe = TickerFrequencyProcessor.process_ticker_frequencies(submission_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_text = 'Is it insider trading if I bought Boeing puts while I am inside the wrecked AAPL? Purely hypothetical of google:  \\nImagine sitting in an airplane when suddenly the KO door blows out.   \\nNow, while everyone is screaming and Yahoo for air, you instead turn on your noise-cancelling head-phones to ignore that crying baby next to you, calmly open your robin-hood app (or whatever broker you prefer, idc), and load up on Boeing puts.   \\nThere is no way the market couldve already priced that in, it is literally just happening.  \\nWould that be considered insider trading? I mean you are literally inside that wreck of an airplane...  \\nOn the other hand, one could argue that you are also outside the airplane, given that the door just blew off...  \\n'\n",
    "modelutils = ModelUtils()\n",
    "processed_glove_text = modelutils.preprocess_text(glove_text)\n",
    "result = run_glove_model(processed_glove_text, combined_vec_dict, 0.9)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloveModel = GloveModel()\n",
    "#setup glove model dictionaries\n",
    "filepath = ('glove/glove.6B.50d.txt')\n",
    "glove_model = GloveModel.load_glove_model(filepath)\n",
    "ticker_to_vector, title_to_vector, title_lookup = GloveModel.create_vector_dicts(company_data, glove_model, vector_size=50)\n",
    "combined_vec_dict = GloveModel.merge_vector_dicts(ticker_to_vector, title_to_vector, title_lookup)\n",
    "\n",
    "#run the extraction model on the tokenised words\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: run_glove_model(tokens, combined_vec_dict, 0.9))\n",
    "\n",
    "#Get top x tickers from each submission\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: TickerFrequencyProcessor.top_tickers(tokens,5))\n",
    "\n",
    "# create ticker df for streamlit graph\n",
    "ticker_dataframe = TickerFrequencyProcessor.process_ticker_frequencies(submission_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New ticker isolation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fraserlevick/anaconda3/envs/meme_stock_dev/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from extraction_models import GloveModel, FuzzModel, RegexExtraction\n",
    "from utils import ModelUtils, TickerFrequencyProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_reddit_data = 'AGI and PVG, efficient companies with lots of GME interest As you may AMC, precious metals AMC are very manipulated by GME and AGI and tesla familiar with the matter know it:'\n",
    "mini_reddit_answers = ['agi', 'pvg', 'gme', 'amc', 'amc', 'gme', 'agi', 'tsla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_data = ModelUtils.get_company_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tickers from the dictionary\n",
    "company_ticks = {v['ticker'] for k, v in company_data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load reddit data\n",
    "reddit_tokens = ModelUtils.preprocess_text(mini_reddit_data, lower_case = True)\n",
    "reddit_tokens_norm = ModelUtils.preprocess_text(mini_reddit_data, lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fraserlevick/anaconda3/envs/meme_stock_dev/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from get_reddit import RedditSubmissions, RedditAPIHelper\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from extraction_models import GloveModel, FuzzModel, RegexExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_comma_separated_string_to_list(variable):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Convert a comma-separated string into a list of strings.\n",
    "\n",
    "    Arguments:\n",
    "        variable: Comma-separated string\n",
    "    \n",
    "    Output:\n",
    "        List of strings\n",
    "    \"\"\"\n",
    "    # Remove leading and trailing whitespace, then split by comma and strip each element\n",
    "    list_of_strings = [item.strip() for item in variable.split(',')]\n",
    "    return list_of_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from file\n",
    "test_data_file = 'test_data/reddit_test_data.txt'\n",
    "actual_values_file = 'test_data/reddit_data_answers.txt'\n",
    "actual_tickervalues_file = 'test_data/reddit_answers_tickers.txt'\n",
    "\n",
    "#load test files into variable\n",
    "test_data = ModelUtils.load_text_files(test_data_file)\n",
    "true_values = ModelUtils.load_text_files(actual_values_file)\n",
    "true_values_tickers = ModelUtils.load_text_files(actual_tickervalues_file)\n",
    "\n",
    "#convert results to list of 'tokens' for performance_evaluation function\n",
    "true_values_list = convert_comma_separated_string_to_list(true_values)\n",
    "true_values_tickers_list = convert_comma_separated_string_to_list(true_values_tickers)\n",
    "\n",
    "#load company data from public dictionary to make look-up dictionaries/lists\n",
    "company_data = ModelUtils.get_company_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List of tickers\n",
    "ticker_list = RegexExtraction.create_ticker_list(company_data)\n",
    "\n",
    "# process the test data into tokens\n",
    "tokens_upper = ModelUtils.preprocess_text(test_data, lower_case = False)\n",
    "\n",
    "# run the RegexExtraction method and extract the tickers and time to identify each ticker\n",
    "regex_tickers, regex_time = RegexExtraction.extract_tickers(tokens_upper, ticker_list)\n",
    "\n",
    "# Evaluate the precision and sensitivity of the ticker_extraction method \n",
    "regex_precision, regex_sensitivity = ModelUtils.evaluate_model_performance(true_values_tickers_list, regex_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9444444444444444, 0.8360655737704918)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_precision, regex_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GME',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'AMC',\n",
       " 'NOK',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AG',\n",
       " 'SLV',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'JPM',\n",
       " 'GME',\n",
       " 'SLV',\n",
       " 'AG',\n",
       " 'AG',\n",
       " 'SLV',\n",
       " 'AG',\n",
       " 'NOK',\n",
       " 'GO',\n",
       " 'GME',\n",
       " 'NOK',\n",
       " 'PLTR',\n",
       " 'BB',\n",
       " 'GME',\n",
       " 'NOK',\n",
       " 'SU',\n",
       " 'AMC',\n",
       " 'TSLA',\n",
       " 'TSLA',\n",
       " 'DDS',\n",
       " 'DDS',\n",
       " 'GME',\n",
       " 'USA',\n",
       " 'GME',\n",
       " 'NOK',\n",
       " 'GME',\n",
       " 'BB',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'BB',\n",
       " 'AMC',\n",
       " 'NOK',\n",
       " 'SLV',\n",
       " 'PSLV',\n",
       " 'CTRM',\n",
       " 'VALE',\n",
       " 'ZOM',\n",
       " 'AGI']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GME',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'BB',\n",
       " 'AMC',\n",
       " 'NOK',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'AG',\n",
       " 'SLV',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'JPM',\n",
       " 'GME',\n",
       " 'SLV',\n",
       " 'AG',\n",
       " 'AG',\n",
       " 'SLV',\n",
       " 'AG',\n",
       " 'NOK',\n",
       " 'GME',\n",
       " 'NOK',\n",
       " 'PLTR',\n",
       " 'BB',\n",
       " 'GME',\n",
       " 'NOK',\n",
       " 'SU',\n",
       " 'RIDE',\n",
       " 'AMC',\n",
       " 'TSLA',\n",
       " 'TSLA',\n",
       " 'DDS',\n",
       " 'DDS',\n",
       " 'GME',\n",
       " 'GME',\n",
       " 'DFV',\n",
       " 'NOK',\n",
       " 'GME',\n",
       " 'BB',\n",
       " 'AMC',\n",
       " 'GME',\n",
       " 'BB',\n",
       " 'AMC',\n",
       " 'NOK',\n",
       " 'NAKD',\n",
       " 'PSLV',\n",
       " 'NKD',\n",
       " 'CTRM',\n",
       " 'VALE',\n",
       " 'VALE',\n",
       " 'ZOM',\n",
       " 'WPG',\n",
       " 'WPG',\n",
       " 'AGI',\n",
       " 'PVG']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_values_tickers_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load company data\n",
    "company_data = modelutils.get_company_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 2: fuzz method evaluation\n",
    "fuzz_model = FuzzModel()\n",
    "title_to_ticker, ticker_to_title, combined_dict = fuzz_model.create_fuzz_dicts(company_data)\n",
    "values_to_key, values = fuzz_model.fuzz_preprocess(combined_dict)\n",
    "fuzz_result = fuzz_model.fuzz_optimum_threshold(reddit_tokens, values_to_key, values, mini_reddit_answers, thresholds = np.arange(70, 101, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzz_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 3: GloVe method evaluation\n",
    "filepath = ('glove/glove.6B.50d.txt')\n",
    "glove_model = GloveModel.load_glove_model(filepath)\n",
    "ticker_to_vector, title_to_vector, title_lookup = GloveModel.create_vector_dicts(company_data, glove_model, vector_size=50)\n",
    "combined_vec_dict = GloveModel.merge_vector_dicts(ticker_to_vector, title_to_vector, title_lookup)\n",
    "glove_result = GloveModel.glove_optimum_threshold(reddit_tokens, mini_reddit_answers, combined_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_text_files('test_data.txt')\n",
    "test_title_answers = load_text_files('test_company_answers.txt')\n",
    "test_ticker_answers = load_text_files('ticker_answers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    list_backup = list \n",
    "except TypeError:\n",
    "    # If the above line throws a TypeError, it means list has been overridden\n",
    "    del list  # Delete the overridden variable\n",
    "    list = list_backup  # Restore the built-in list function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_data = load_text_files('reddit_test_data.txt')\n",
    "reddit_data_answers = load_text_files('reddit_data_answers.txt')\n",
    "reddit_tokens = preprocess_text(reddit_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep fuzzy dictionaries\n",
    "reddit_key_values, reddit_values = fuzz_preprocess(combined_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_reddit_data = 'AGI and PVG, efficient companies with lots of GME interest As you may AMC, precious metals AMC are very manipulated by GME and AGI and tesla familiar with the matter know it:'\n",
    "mini_reddit_answers = ['agi', 'pvg', 'gme', 'amc', 'amc', 'gme', 'agi', 'tsla']\n",
    "m_red_tokens = preprocess_text(mini_reddit_data, lower_case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_method = extract_tickers(m_red_tokens, blacklist)\n",
    "reg_precision, reg_sensitivity = evaluate_model_performance(mini_reddit_answers, regex_method)\n",
    "reg_precision, reg_sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_red_tokens = ['agi', 'pvg', 'efficient','companies', 'lots','gme', 'interest', 'may', 'amc', 'precious', 'metals', 'amc', 'manipulated', 'gme', 'agi', 'tesla', 'familiar', 'matter','know']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a line chart\n",
    "color_discrete_sequence = px.colors.qualitative.Plotly\n",
    "fig = px.scatter(tickers, x='date', y='count', color='ticker',\n",
    "              title='Ticker Mention Frequency Over Time', color_discrete_sequence=color_discrete_sequence)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run your Streamlit app\n",
    "command = [\"streamlit\", \"run\", \"path_to_your_python_file/web_app.py\"]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select ticker & get info and trading volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the relative path to the JSON file\n",
    "file_path = 'company_tickers.json'\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "cleaned_data = [{'ticker': entry[\"ticker\"], 'company': entry['title']} for entry in data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')  # Download the necessary resources for tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ticker_counts.plot(kind='line', ax=ax)\n",
    "ax.set_title('Ticker Mention Frequency by Date')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Ticker')\n",
    "fig.subplots_adjust(bottom=0.2, top=0.9)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# cumulative_mentions.plot(kind='line', ax=ax)\n",
    "# ax.set_title('Ticker Mention Frequency by Date')\n",
    "# ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Frequency')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.legend(title='Ticker')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to figure out if we are missing daily discussion threads\n",
    "* missing february 19, 2024 https://www.reddit.com/r/wallstreetbets/comments/1aukpdc/daily_discussion_thread_for_february_19_2024/\n",
    "* why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "march_discussions = [thread for thread in list_a if 'February' in thread]\n",
    "march_discussions\n",
    "def extract_date(thread_title):\n",
    "    match = re.search(r'Daily Discussion Thread for (\\w+ \\d{1,2}, \\d{4})', thread_title)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(1), '%B %d, %Y')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "sorted_threads = sorted(march_discussions, key=extract_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure i stay in the request limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 options\n",
    "1. Get stock tickers using by cleaning words individually \n",
    "   1. Simple, but miss everytime someone says the name of the company\n",
    "   2. https://medium.com/@financial_python/how-to-get-trending-stock-tickers-from-reddit-using-praw-and-python-1fccc7f06748\n",
    "2. Use NER: named entity recognition - lightweight learning library\n",
    "3. To machine learning with NLP libraries - complex\n",
    "\n",
    "Decision: do NER for now. Maybe upgrade to machine learning NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ticker extraction with Regex\n",
    "* use re.compile if reusing the pattern multiple times in a program\n",
    "* Issue is some text like 'A' and 'I' that are technically tickers, are not being implied as such in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "org_list = []\n",
    "\n",
    "for entity in doc.ents:\n",
    "    if entity.label_ == 'ORG':\n",
    "        org_list.append(entity.text)\n",
    "    else:\n",
    "        None\n",
    "\n",
    "org_list = list(set(org_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme_app_dev.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
