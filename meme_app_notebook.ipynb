{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Meme Stock App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "1. Importing Libraries\n",
    "2. Getting data from Reddit subreddits\n",
    "3. Cleaning text data to extract companies and stock tickers\n",
    "   1. Regex method\n",
    "   2. Word2vec method\n",
    "4. Streamlit webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant libraries\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import re\n",
    "import pandas as pd \n",
    "from collections import Counter \n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import streamlit as st\n",
    "import plotly.express as px\n",
    "import yfinance as yf\n",
    "from plotly import graph_objs as go\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from gensim.models import Word2Vec #pip install updated gensim library from github, new release to fix bug due soon. Gensim v10.2 has bug (scipy deprecated 'triu' from 'scipy.linalg', triu required for gensim v10.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting data from Reddit\n",
    "#### Webscraper vs Reddit API\n",
    "Scraping data from reddit is employed regularly, therefore creating a webscraper-based data miner was investigated. The decision to move towards using Reddit API was made because Reddit actively employs anti-webscraping measures. To preserve the long-lasting function of the application, API calls a developer account was created and it's credentials stored in a .env file. The limitation of using reddit API (PRAW library) rather than a web-scraper is that reddit enforces an API call limit of 60 calls per minute. Because of this, an API_limit function was created to investigate the number of API calls the code was calling per run. Certain PRAW functions were avoided that used multiple API calls, and the current code is well within the API call limit enforced. As such, the decision to keep the API calls as the method of data mining was kept.\n",
    "\n",
    "#### Reddit API\n",
    "Reddit maintains an API library for obtaining data from reddit submissions and subreddits. Any subreddit, such as r/wallstreetbets, can be specified and submissions (otherwise known as *posts*) with relevant data (title, text, upvotes, etc) can be accessed. Top submissions can be accessed with a limit of 1000 submissions. Comments from submissions can also be accessed via a CommentForest instance, which is initiated alongside a reddit instance. \n",
    "\n",
    "#### Submissions vs Daily Discussions\n",
    "Submissions are general posts posted within Wallstreetbets. On the other hand Daily Discussions are daily posts whereby active users comment on stocks they are looking at on the day. Both submissions and comments from Daily Discussions were mined for text data. Other useful parameters such as upvote count, number of comments, author, time, and others were also extracted in case a use case arose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access top submissions from r/Wallstreetbets without submission limit and within a time range of 1 year\n",
    "top_submissions = get_top_submissions('wallstreetbets', limit=None, time_filter='year')\n",
    "\n",
    "#Extract data from reddit.submission object such as title, text, author, date, upvote_ratio, etc. \n",
    "top_submission_data = get_submission_data(top_submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning Data\n",
    "\n",
    "### Data Characteristics\n",
    "The submissions and comment data is very unformatted and diverse, consisting of links, emojis, uppercase, lowercase, uneven spacing, mispellings, and others. On brief observation, most posts that mention specific companies use 'tickers', or ...!! explain what a ticker is!! however some posts write out the full company. \n",
    "\n",
    "As such, it was quite difficult to interpret and extract company names and tickers. Company names are hard to extract because they follow no distinct linguistic pattern. Many company names consist of multiple words, such as 'Bank of America' or 'Morgan Stanley', whilst others are noun words of any length, which are indistiguishable from non-company nouns. On the other hand, tickers are easier to extract because they have a distinct pattern that consists of 1-5 capitalised letters. Nonetheless, extracting tickers presented many challenges as many submissions and comments do not always capitalise tickers, often contain various acronyms, and other capitalised words and sentences for linguistic emphasis.  \n",
    "\n",
    "### Data preparation:\n",
    "The first step was to remove all punctuation and patterns such as /n (represents space). From the ntlk library, all semantic stopwords were removed **Explain more**, along with punctuation, http links, emoji's, and '/n' which marks the start of a new line?. \n",
    "\n",
    "The next step is to extract tickers and/or company names mentioned in each post or comment. There were three possible methods; regex, NER, and a machine learning method. To test which method is the best, a 'testing text' was created from reddit comments and submissions that contained 100 tickers and 100 company names mentioned in the text. Below each method is described and the test results explained. \n",
    "\n",
    "### Testing Method\n",
    "\n",
    "### Regex method\n",
    "From the cleaned text, the first method was to extract all the words that matched the pattern of 1-5 capital letters, and to remove any words manually added to a blacklist. The rationale behind this method was that most posts observed used tickers when discussing the company, therefore it could meet the requirements needed ?!(what requirements neede?!) As expected, no company names were extracted.\n",
    "\n",
    "results\n",
    "\n",
    "### NER\n",
    "NER or named entity recognition is a method derived from Spacy... the benefit from regex. it was attempted. \n",
    "\n",
    "results were\n",
    "\n",
    "### Word2Vec\n",
    "* Word2Vec was used because the app required a more customised data cleaning method to identify companeis and tickers. THe benefit was you can calculate the similarity which means it covers for mispelled words, etc.  the difficult \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO section\n",
    "1. testing - create submission post of 100 tickers and 100 companies to see which method extracted the most against the list\n",
    "*come up with rating for each method - compare against company list to see how many are recognised\n",
    "2. Create text_cleaning function\n",
    "   1. remove punctuation\n",
    "   2. remove stopwords\n",
    "   3. remove '/n'\n",
    "   4. remove links\n",
    "   5. remove emoji's \n",
    "3. create testing function\n",
    "   1. create submission text test\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data Display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme_app_dev.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
