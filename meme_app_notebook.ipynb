{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Meme Stock App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "1. Importing Libraries\n",
    "2. Investigating Optimum Ticker Extraction Method\n",
    "   1. Regex Method\n",
    "   2. String-Matching (FuzzyWuzzy)\n",
    "   3. Word Vectorisaiton (GloVe)\n",
    "3. Getting data from Reddit subreddits\n",
    "4. Cleaning text data to extract companies and stock tickers\n",
    "   1. Regex method\n",
    "   2. Word2vec method\n",
    "5. Streamlit webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fraserlevick/anaconda3/envs/meme_stock_dev/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from get_reddit import RedditSubmissions, RedditAPIHelper\n",
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from extraction_models import GloveModel, FuzzModel, RegexExtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import relevant libraries\n",
    "# import praw\n",
    "# from datetime import datetime\n",
    "# import pytz\n",
    "# import re\n",
    "# import pandas as pd \n",
    "# from collections import Counter \n",
    "# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "# import streamlit as st\n",
    "# import plotly.express as px\n",
    "# import yfinance as yf\n",
    "# from plotly import graph_objs as go\n",
    "# import json\n",
    "# import nltk\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# #from gensim.models import Word2Vec #pip install updated gensim library from github, new release to fix bug due soon. Gensim v10.2 has bug (scipy deprecated 'triu' from 'scipy.linalg', triu required for gensim v10.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ticker & Company Extraction Method Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from file\n",
    "test_data_file = 'test_data/reddit_test_data.txt'\n",
    "actual_values_file = 'test_data/reddit_data_answers.txt'\n",
    "actual_tickervalues_file = 'test_data/reddit_answers_tickers.txt'\n",
    "\n",
    "#load test files into variable\n",
    "test_data = ModelUtils.load_text_files(test_data_file)\n",
    "true_values = ModelUtils.load_text_files(actual_values_file)\n",
    "true_values_tickers = ModelUtils.load_text_files(actual_tickervalues_file)\n",
    "\n",
    "#convert results to list of 'tokens' for performance_evaluation function\n",
    "true_values_list = ModelUtils.convert_comma_separated_string_to_list(true_values)\n",
    "true_values_tickers_list = ModelUtils.convert_comma_separated_string_to_list(true_values_tickers)\n",
    "\n",
    "#load company data from public dictionary to make look-up dictionaries/lists\n",
    "company_data = ModelUtils.get_company_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Regex Extraction\n",
    "To begin with this method, we first need to create a list of company tickers from the company data. The regex extraction method requires that the tokenised words are not made into lowercase, and this is accounted for in the preprocess_text function. The extract_tickers function evaluates each token and if it fits the pattern of a ticker (i.e. 1-5 capitalised letters in a row) and matches a ticker in the list of tickers, it is considered a match. The matches are compared against a true_value_list of tickers extracted from the sample reddit text. Unfortunately, this method had different true_values than the values that validate the string matching and vectorisation methods because this method can only identify tickers and not companies. This makes it hard to compare the regex model against the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List of tickers\n",
    "ticker_list = RegexExtraction.create_ticker_list(company_data)\n",
    "\n",
    "# process the test data into tokens\n",
    "tokens_upper = ModelUtils.preprocess_text(test_data, lower_case = False)\n",
    "\n",
    "# run the RegexExtraction method and extract the tickers and time to identify each ticker\n",
    "regex_tickers, regex_time = RegexExtraction.extract_tickers(tokens_upper, ticker_list)\n",
    "\n",
    "# Evaluate the precision and sensitivity of the ticker_extraction method \n",
    "regex_precision, regex_sensitivity = ModelUtils.evaluate_model_performance(true_values_tickers_list, regex_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers identified: ['GME', 'GME', 'GME', 'AMC', 'GME', 'AMC', 'NOK', 'GME', 'GME', 'AMC', 'GME', 'GME', 'AG', 'SLV', 'GME', 'GME', 'JPM', 'GME', 'SLV', 'AG', 'AG', 'SLV', 'AG', 'NOK', 'GO', 'GME', 'NOK', 'PLTR', 'BB', 'GME', 'NOK', 'SU', 'AMC', 'TSLA', 'TSLA', 'DDS', 'DDS', 'GME', 'USA', 'GME', 'NOK', 'GME', 'BB', 'AMC', 'GME', 'BB', 'AMC', 'NOK', 'SLV', 'PSLV', 'CTRM', 'VALE', 'ZOM', 'AGI']\n",
      "Time taken per ticker: 3.7037853627477973e-05s\n",
      "Precision of Regex Model: 0.944\n",
      "Sensitivity of Regex Model: 0.836\n"
     ]
    }
   ],
   "source": [
    "print(f'Tickers identified: {regex_tickers}')\n",
    "print(f'Time taken per ticker: {regex_time}s')\n",
    "print(f'Precision of Regex Model: {regex_precision:.3f}')\n",
    "print(f'Sensitivity of Regex Model: {regex_sensitivity:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Extraction Results\n",
    "The results show that this method is fast, taking 3.70 e-05 seconds. This is primarily due to the low algorithmic complexity of this algorithm. Moreover, this method was very precise, correctly identifying 94.4% true values. The model also had a sensitivity of 83%, meaning it identified 83% of the true values from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: String-matching Extraction (FuzzyWuzzy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting data from Reddit\n",
    "#### Webscraper vs Reddit API\n",
    "Scraping data from reddit is employed regularly, therefore creating a webscraper-based data miner was investigated. The decision to move towards using Reddit API was made because Reddit actively employs anti-webscraping measures. To preserve the long-lasting function of the application, API calls a developer account was created and it's credentials stored in a .env file. The limitation of using reddit API (PRAW library) rather than a web-scraper is that reddit enforces an API call limit of 60 calls per minute. Because of this, an API_limit function was created to investigate the number of API calls the code was calling per run. Certain PRAW functions were avoided that used multiple API calls, and the current code is well within the API call limit enforced. As such, the decision to keep the API calls as the method of data mining was kept.\n",
    "\n",
    "#### Reddit API\n",
    "Reddit maintains an API library for obtaining data from reddit submissions and subreddits. Any subreddit, such as r/wallstreetbets, can be specified and submissions (otherwise known as *posts*) with relevant data (title, text, upvotes, etc) can be accessed. Top submissions can be accessed with a limit of 1000 submissions. Comments from submissions can also be accessed via a CommentForest instance, which is initiated alongside a reddit instance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv('credentials.env')\n",
    "\n",
    "# Get the credentials from the .env file\n",
    "user_agent = os.getenv('USER_AGENT')\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "\n",
    "# Initialize the Reddit API instance\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions vs Daily Discussions\n",
    "Submissions are general posts posted within Wallstreetbets. On the other hand Daily Discussions are daily posts whereby active users comment on stocks they are looking at on the day. Both submissions and comments from Daily Discussions were mined for text data. Other useful parameters such as upvote count, number of comments, author, time, and others were also extracted in case a use case arose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RedditSubmissions with the Reddit API instance\n",
    "reddit_submissions = RedditSubmissions(reddit)\n",
    "\n",
    "#Access top submissions from r/Wallstreetbets without submission limit\n",
    "submissions = reddit_submissions.get_submissions('wallstreetbets',sort = 'top', limit=None, time_filter='year')\n",
    "\n",
    "#Extract data from reddit.submission object such as title, text, author, date, upvote_ratio, etc. \n",
    "submission_data = reddit_submissions.extract_submission_data(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning Data\n",
    "\n",
    "### Data Characteristics\n",
    "The submissions and comment data is very unformatted and diverse, consisting of links, emojis, uppercase, lowercase, uneven spacing, mispellings, and others. On brief observation, most posts that mention specific companies use 'tickers', or ...!! explain what a ticker is!! however some posts write out the full company. \n",
    "\n",
    "As such, it was quite difficult to interpret and extract company names and tickers. Company names are hard to extract because they follow no distinct linguistic pattern. Many company names consist of multiple words, such as 'Bank of America' or 'Morgan Stanley', whilst others are noun words of any length, which are indistiguishable from non-company nouns. On the other hand, tickers are easier to extract because they have a distinct pattern that consists of 1-5 capitalised letters. Nonetheless, extracting tickers presented many challenges as many submissions and comments do not always capitalise tickers, often contain various acronyms, and other capitalised words and sentences for linguistic emphasis.  \n",
    "\n",
    "### Data preparation:\n",
    "The first step was to remove all punctuation and patterns such as /n (represents space). From the ntlk library, all semantic stopwords were removed **Explain more**, along with punctuation, http links, emoji's, and '/n' which marks the start of a new line?. \n",
    "\n",
    "The next step is to extract tickers and/or company names mentioned in each post or comment. There were three possible methods; regex, NER, and a machine learning method. To test which method is the best, a 'testing text' was created from reddit comments and submissions that contained 100 tickers and 100 company names mentioned in the text. Below each method is described and the test results explained. \n",
    "\n",
    "### Testing Method\n",
    "\n",
    "### Regex method\n",
    "From the cleaned text, the first method was to extract all the words that matched the pattern of 1-5 capital letters, and to remove any words manually added to a blacklist. The rationale behind this method was that most posts observed used tickers when discussing the company, therefore it could meet the requirements needed ?!(what requirements neede?!) As expected, no company names were extracted.\n",
    "\n",
    "results\n",
    "\n",
    "### FuzzyWuzzy\n",
    "NER or named entity recognition is a method derived from Spacy... the benefit from regex. it was attempted. \n",
    "\n",
    "results were\n",
    "\n",
    "### GloVe\n",
    "* Word2Vec was used because the app required a more customised data cleaning method to identify companies and tickers. THe benefit was you can calculate the similarity which means it covers for mispelled words, etc.  the difficult \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Data Display "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme_app_dev.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
