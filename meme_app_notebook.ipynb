{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Meme Stock App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Table of contents\n",
    "1. Importing Libraries\n",
    "2. Getting data from Reddit subreddits\n",
    "3. Cleaning Data\n",
    "4. Investigating Optimum Ticker Extraction Method\n",
    "   1. Regex Method\n",
    "   2. String-Matching (FuzzyWuzzy)\n",
    "   3. Word Vectorisaiton (GloVe)\n",
    "5. \n",
    "6. Cleaning text data to extract companies and stock tickers\n",
    "   1. Regex method\n",
    "   2. Word2vec method\n",
    "7. Streamlit webapp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fraserlevick/anaconda3/envs/meme_stock_dev/lib/python3.11/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from utils import *\n",
    "from extraction_models import GloveModel, FuzzModel, RegexExtraction\n",
    "from get_reddit import RedditSubmissions, RedditAPIHelper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting data from Reddit\n",
    "#### Webscraper vs Reddit API\n",
    "Scraping data from reddit is employed regularly, therefore creating a webscraper-based data miner was investigated. The decision to move towards using Reddit API was made because Reddit actively employs anti-webscraping measures. To preserve the long-lasting function of the application, API calls a developer account was created and it's credentials stored in a .env file. The limitation of using reddit API (PRAW library) rather than a web-scraper is that reddit enforces an API call limit of 60 calls per minute. Because of this, an API_limit function was created to investigate the number of API calls the code was calling per run. Certain PRAW functions were avoided that used multiple API calls, and the current code is well within the API call limit enforced. As such, the decision to keep the API calls as the method of data mining was kept.\n",
    "\n",
    "#### Reddit API\n",
    "Reddit maintains an API library for obtaining data from reddit submissions and subreddits. Any subreddit, such as r/wallstreetbets, can be specified and submissions (otherwise known as *posts*) with relevant data (title, text, upvotes, etc) can be accessed. Top submissions can be accessed with a limit of 1000 submissions. Comments from submissions can also be accessed via a CommentForest instance, which is initiated alongside a reddit instance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv('credentials.env')\n",
    "\n",
    "# Get the credentials from the .env file\n",
    "user_agent = os.getenv('USER_AGENT')\n",
    "client_id = os.getenv('CLIENT_ID')\n",
    "client_secret = os.getenv('CLIENT_SECRET')\n",
    "\n",
    "# Initialize the Reddit API instance\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions vs Daily Discussions\n",
    "Submissions are general posts posted within Wallstreetbets. On the other hand Daily Discussions are daily posts whereby active users comment on stocks they are looking at on the day. Both submissions and comments from Daily Discussions were mined for text data. Other useful parameters such as upvote count, number of comments, author, time, and others were also extracted in case a use case arose. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RedditSubmissions with the Reddit API instance\n",
    "reddit_submissions = RedditSubmissions(reddit)\n",
    "\n",
    "#Access top submissions from r/Wallstreetbets without submission limit\n",
    "submissions = reddit_submissions.get_submissions('wallstreetbets',sort = 'top', limit=None, time_filter='year')\n",
    "\n",
    "#Extract data from reddit.submission object such as title, text, author, date, upvote_ratio, etc. \n",
    "submission_data = reddit_submissions.extract_submission_data(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning Data\n",
    "\n",
    "### Data Characteristics\n",
    "The submissions and comment data is very unformatted and diverse, consisting of links, emojis, uppercase, lowercase, uneven spacing, mispellings, and others. On brief observation, most posts that mention specific companies use 'tickers', or ...!! explain what a ticker is!! however some posts write out the full company. \n",
    "\n",
    "As such, it was quite difficult to interpret and extract company names and tickers. Company names are hard to extract because they follow no distinct linguistic pattern. Many company names consist of multiple words, such as 'Bank of America' or 'Morgan Stanley', whilst others are noun words of any length, which are indistiguishable from non-company nouns. On the other hand, tickers are easier to extract because they have a distinct pattern that consists of 1-5 capitalised letters. Nonetheless, extracting tickers presented many challenges as many submissions and comments do not always capitalise tickers, often contain various acronyms, and other capitalised words and sentences for linguistic emphasis.  \n",
    "\n",
    "### Data preparation:\n",
    "The first step was to remove all punctuation and patterns such as /n (represents space). From the ntlk library, all semantic stopwords were removed **Explain more**, along with punctuation, http links, emoji's, and '/n' which marks the start of a new line?. \n",
    "\n",
    "The next step is to extract tickers and/or company names mentioned in each post or comment. There were three possible methods; regex, NER, and a machine learning method. To test which method is the best, a 'testing text' was created from reddit comments and submissions that contained 100 tickers and 100 company names mentioned in the text. Below each method is described and the test results explained. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and tokenise the words from each post in the dataframe\n",
    "submission_data['tickers'] = submission_data['all_text'].apply(lambda x: ModelUtils.preprocess_text(x, lower_case = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Ticker & Company Extraction Method Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load company data for look-up dictionaries for the different models\n",
    "Test data is loaded using load_text_files, and then true_values files converted to lists to be used in evaluate_model_performance function. Load the company data from the json file downloaded from the SEC. This file should contain all the companies currently listed in the USA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data from file\n",
    "test_data_file = 'test_data/reddit_test_data.txt'\n",
    "actual_values_file = 'test_data/reddit_data_answers.txt'\n",
    "actual_tickervalues_file = 'test_data/reddit_answers_tickers.txt'\n",
    "\n",
    "#load test files into variable\n",
    "test_data = ModelUtils.load_text_files(test_data_file)\n",
    "true_values = ModelUtils.load_text_files(actual_values_file)\n",
    "true_values_tickers = ModelUtils.load_text_files(actual_tickervalues_file)\n",
    "\n",
    "#convert results to list of 'tokens' for performance_evaluation function\n",
    "true_values_list = ModelUtils.convert_comma_separated_string_to_list(true_values)\n",
    "true_values_tickers_list = ModelUtils.convert_comma_separated_string_to_list(true_values_tickers)\n",
    "\n",
    "#load company data from public dictionary to make look-up dictionaries/lists\n",
    "company_data = ModelUtils.get_company_information()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Regex Extraction\n",
    "To begin with this method, we first need to create a list of company tickers from the company data. The regex extraction method requires that the tokenised words are not made into lowercase, and this is accounted for in the preprocess_text function. The extract_tickers function evaluates each token and if it fits the pattern of a ticker (i.e. 1-5 capitalised letters in a row) and matches a ticker in the list of tickers, it is considered a match. The matches are compared against a true_value_list of tickers extracted from the sample reddit text. Unfortunately, this method had different true_values than the values that validate the string matching and vectorisation methods because this method can only identify tickers and not companies. This makes it hard to compare the regex model against the other two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create List of tickers\n",
    "ticker_list = RegexExtraction.create_ticker_list(company_data)\n",
    "\n",
    "# process the test data into tokens\n",
    "tokens_upper = ModelUtils.preprocess_text(test_data, lower_case = False)\n",
    "\n",
    "# run the RegexExtraction method and extract the tickers and time to identify each ticker\n",
    "regex_tickers, regex_time = RegexExtraction.extract_tickers(tokens_upper, ticker_list)\n",
    "\n",
    "# Evaluate the precision and sensitivity of the ticker_extraction method \n",
    "regex_precision, regex_sensitivity = ModelUtils.evaluate_model_performance(true_values_tickers_list, regex_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers identified: ['GME', 'GME', 'GME', 'AMC', 'GME', 'AMC', 'NOK', 'GME', 'GME', 'AMC', 'GME', 'GME', 'AG', 'SLV', 'GME', 'GME', 'JPM', 'GME', 'SLV', 'AG', 'AG', 'SLV', 'AG', 'NOK', 'GO', 'GME', 'NOK', 'PLTR', 'BB', 'GME', 'NOK', 'SU', 'AMC', 'TSLA', 'TSLA', 'DDS', 'DDS', 'GME', 'USA', 'GME', 'NOK', 'GME', 'BB', 'AMC', 'GME', 'BB', 'AMC', 'NOK', 'SLV', 'PSLV', 'CTRM', 'VALE', 'ZOM', 'AGI']\n",
      "Time taken per ticker: 3.7037853627477973e-05s\n",
      "Precision of Regex Model: 0.944\n",
      "Sensitivity of Regex Model: 0.836\n"
     ]
    }
   ],
   "source": [
    "print(f'Tickers identified: {regex_tickers}')\n",
    "print(f'Time taken per ticker: {regex_time}s')\n",
    "print(f'Precision of Regex Model: {regex_precision:.3f}')\n",
    "print(f'Sensitivity of Regex Model: {regex_sensitivity:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex Extraction Results\n",
    "The results show that this method is fast, taking 3.70 e-05 seconds. This is primarily due to the low algorithmic complexity of this algorithm. Moreover, this method was very precise, correctly identifying 94.4% true values. The model also had a sensitivity of 83%, meaning it identified 83% of the true values from the text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: String-matching Extraction (FuzzyWuzzy)\n",
    "This method matches the string values of the tokenised text with string values of company names and tickers. This required a dictionary with the tickers as keys, and company names and tickers as string values. This was done by converting company_data file into three lists; ticker_to_title, title_to_ticker and merging them to form a combined dictionary. This also required some data cleaning via clean_titles function in order to remove company suffixes. This combined dictionary was the preprocessed into a list of all possible values (called values) and a dictionary with every key value pair possible (values_to_key). This step was implemented in order to tailor the dataset to best suit the needs of the FuzzyWuzzy model in order to increase efficiency. In the fuzz_best_match function, FuzzyWuzzy calculates the string similarity of the input token against the values list to find the best match. The best match is then compared against an inputted similarity threshold, which allows the user to toggle the precision of the function.\n",
    "\n",
    "In order to run through a list of tokens, this function was implemented into a loop and best matches appended to a list.\n",
    "\n",
    "To evaluate the performance of this method along different thresholds,a function was created to test fuzz_best_match on reddit data with various similarity thresholds. The list of predicted values (i.e. best matches) for each threshold was compared individually to the list of true_values to ascertain time, precision, and sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate fuzz_model class instance\n",
    "fuzz_model = FuzzModel()\n",
    "\n",
    "# Create 'fuzz' look-up dictionaries from the company data file\n",
    "title_to_ticker, ticker_to_title, combined_dict = fuzz_model.create_fuzz_dicts(company_data)\n",
    "\n",
    "# Preprocess these dictionaries to tailor them to the process.extractOne(token, values, scorer=fuzz.token_sort_ratio) fuzz model.\n",
    "values_to_key, values = fuzz_model.fuzz_preprocess(combined_dict)\n",
    "\n",
    "#preprocess the text data into tokens\n",
    "word_tokens = ModelUtils.preprocess_text(test_data)\n",
    "\n",
    "# Run fuzz optimum threshold analysis against reddit data to oberve time, precision, and sensitivtiy at varying thresholds\n",
    "fuzz_result = fuzz_model.fuzz_optimum_threshold(word_tokens, values_to_key, values, true_values_list, thresholds = np.arange(70, 101, 5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Threshold: 70, precision: 0.157, sensitivity: 0.776, time taken to match: 0.5981s',\n",
       " 'Threshold: 75, precision: 0.180, sensitivity: 0.776, time taken to match: 0.5612s',\n",
       " 'Threshold: 80, precision: 0.250, sensitivity: 0.776, time taken to match: 0.5520s',\n",
       " 'Threshold: 85, precision: 0.337, sensitivity: 0.776, time taken to match: 0.6092s',\n",
       " 'Threshold: 90, precision: 0.611, sensitivity: 0.763, time taken to match: 0.4850s',\n",
       " 'Threshold: 95, precision: 0.659, sensitivity: 0.763, time taken to match: 0.5166s',\n",
       " 'Threshold: 100, precision: 0.659, sensitivity: 0.763, time taken to match: 0.5941s']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3: Word Vectorisation Extraction (GloVe Model)\n",
    "This method uses word vectorization with the GloVe (Global Vectors for Word Representation) model to extract company titles and tickers from text data. The GloVe model, developed by Jeffrey Pennington from standford was developed in 2014, and represents words as vectors which can capture semantic relationships.\n",
    "\n",
    "The GloVe model is first loaded from a pre-trained file and converted to a dictionary where words are keys and their corresponding vectors are values. \n",
    "\n",
    "Three dictionaries are created so that vectors can be looked up as values and the key represented the match. Vectorised dictionaries are created by splitting the text into words, and retrieving vectors for these words from the GloVe model. For  companies comprised of multiple words, the average of these vectors is computed for to represent the entire text as a single vector. the three dictionaries are:\n",
    "\n",
    "ticker_to_vector: Maps tickers to their vector representations.\n",
    "title_to_vector: Maps company titles to their vector representations.\n",
    "title_lookup dictionary: Maps tickers to their corresponding company titles.\n",
    "\n",
    "From these dictionaries the vectors for both tickers and titles are combined into a single dictionary called combined_vec_dict. Each ticker maps to a tuple containing its own vector and the vector of its corresponding company title. \n",
    "\n",
    "To find a vector match, the glove_best_match function calculates the cosine similarity between the vector of a tokenized word and the vectors in the combined_vec_dict. The ticker with the highest similarity above a specified threshold is selected as the best match.\n",
    "\n",
    "To determine the best threshold for matching, the glove_optimum_threshold function tests various thresholds and evaluates performance based on precision and sensitivity. It compares the predicted tickers with true tickers, recording the time taken for matching, precision, and sensitivity for each threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate GloVemodel instance\n",
    "glove = GloveModel()\n",
    "\n",
    "#define pathway to load pre-trained vector model\n",
    "glove_filepath = ('/Users/fraserlevick/Documents/python_code/MScF_sem2_code/meme_stock_app_other/glove/glove.6B.50d.txt')\n",
    "\n",
    "#load the glove model into a variable\n",
    "glove_model = glove.load_glove_model(glove_filepath)\n",
    "\n",
    "#Create vector look-up dictionaries from company data and glove model\n",
    "ticker_to_vector, title_to_vector, title_lookup = glove.create_vector_dicts(company_data, glove_model, vector_size=50)\n",
    "\n",
    "#create combined vector dictionary \n",
    "combined_vec_dict = glove.merge_vector_dicts(ticker_to_vector, title_to_vector, title_lookup)\n",
    "\n",
    "# Run glove optimum threshold analysis against reddit data to oberve time, precision, and sensitivtiy at varying thresholds\n",
    "glove_result = glove.glove_optimum_threshold(word_tokens, true_values_list, combined_vec_dict, thresholds = np.arange(.699, .999, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Threshold: 0.699, precision: 0.170, sensitivity: 0.763, time taken to match: 0.2790s',\n",
       " 'Threshold: 0.749, precision: 0.190, sensitivity: 0.763, time taken to match: 0.2206s',\n",
       " 'Threshold: 0.799, precision: 0.225, sensitivity: 0.763, time taken to match: 0.1832s',\n",
       " 'Threshold: 0.8490000000000001, precision: 0.301, sensitivity: 0.763, time taken to match: 0.1807s',\n",
       " 'Threshold: 0.8990000000000001, precision: 0.411, sensitivity: 0.763, time taken to match: 0.1893s',\n",
       " 'Threshold: 0.9490000000000002, precision: 0.582, sensitivity: 0.750, time taken to match: 0.1469s',\n",
       " 'Threshold: 0.9990000000000002, precision: 0.667, sensitivity: 0.737, time taken to match: 0.1278s']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Tickers from WSB Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run the extraction model on the tokenised words\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: RegexExtraction.extract_tickers(tokens, ticker_list, time_method = False))\n",
    "\n",
    "#Get top x tickers from each submission\n",
    "submission_data['tickers'] = submission_data['tickers'].apply(lambda tokens: TickerFrequencyProcessor.top_tickers(tokens,5))\n",
    "\n",
    "# create ticker df for streamlit graph\n",
    "ticker_dataframe = TickerFrequencyProcessor.process_ticker_frequencies(submission_data)\n",
    "\n",
    "#export ticker_dataframe as csv to safe on memory and increase app efficiency\n",
    "ticker_file_path = 'ticker_frequencies.csv'\n",
    "ticker_dataframe.to_csv(ticker_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Data in Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  You can now view your Streamlit app in your browser.\n",
      "\n",
      "  Local URL: http://localhost:8501\n",
      "  Network URL: http://10.16.14.181:8501\n",
      "\n",
      "  For better performance, install the Watchdog module:\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m command \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstreamlit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweb_app.py\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Run the command\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/meme_stock_dev/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/anaconda3/envs/meme_stock_dev/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/meme_stock_dev/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/meme_stock_dev/lib/python3.11/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/anaconda3/envs/meme_stock_dev/lib/python3.11/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the command to run your Streamlit app\n",
    "command = [\"streamlit\", \"run\", \"web_app.py\"]\n",
    "\n",
    "# Run the command\n",
    "subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meme_app_dev.venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
